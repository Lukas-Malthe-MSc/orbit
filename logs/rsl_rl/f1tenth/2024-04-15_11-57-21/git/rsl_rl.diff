--- git status ---
On branch attention
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   rsl_rl/modules/actor_critic_recurrent.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/rsl_rl/modules/actor_critic_recurrent.py b/rsl_rl/modules/actor_critic_recurrent.py
index 6321ec5..2a12fd1 100644
--- a/rsl_rl/modules/actor_critic_recurrent.py
+++ b/rsl_rl/modules/actor_critic_recurrent.py
@@ -25,6 +25,10 @@ class ActorCriticRecurrent(ActorCritic):
         rnn_hidden_size=256,
         rnn_num_layers=1,
         init_noise_std=1.0,
+        attention=False,
+        attention_type=None,
+        attention_dims=None,
+        attention_heads=None,
         **kwargs,
     ):
         if kwargs:
@@ -42,13 +46,26 @@ class ActorCriticRecurrent(ActorCritic):
             init_noise_std=init_noise_std,
         )
 
+
+        self.use_attention = attention
         activation = get_activation(activation)
 
+        if self.use_attention:
+            assert attention_dims is not None, "Attention dimensions must be provided if attention is enabled"
+            self.attention = CrossAttention(
+                query_dim=attention_dims[0],
+                key_dim=attention_dims[1],
+                value_dim=attention_dims[2],
+                num_heads=attention_heads,
+            )
+
         self.memory_a = Memory(num_actor_obs, type=rnn_type, num_layers=rnn_num_layers, hidden_size=rnn_hidden_size)
         self.memory_c = Memory(num_critic_obs, type=rnn_type, num_layers=rnn_num_layers, hidden_size=rnn_hidden_size)
 
         print(f"Actor RNN: {self.memory_a}")
         print(f"Critic RNN: {self.memory_c}")
+        if self.use_attention:
+            print(f"Attention: {self.attention}")
 
     def reset(self, dones=None):
         self.memory_a.reset(dones)
@@ -95,3 +112,29 @@ class Memory(torch.nn.Module):
         # When the RNN is an LSTM, self.hidden_states_a is a list with hidden_state and cell_state
         for hidden_state in self.hidden_states:
             hidden_state[..., dones, :] = 0.0
+
+
+class CrossAttention(nn.Module):
+    def __init__(self, query_dim, key_dim, value_dim, num_heads=1):
+        super().__init__()
+        self.query_dim = query_dim
+        self.key_dim = key_dim
+        self.value_dim = value_dim
+        self.num_heads = num_heads
+        self.head_dim = query_dim // num_heads
+
+        self.query = nn.Linear(query_dim, query_dim)
+        self.key = nn.Linear(key_dim, key_dim)
+        self.value = nn.Linear(value_dim, value_dim)
+        self.out = nn.Linear(value_dim, query_dim)
+
+    def forward(self, query, key, value):
+        batch_size = query.size(0)
+        query = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
+        key = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
+        value = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
+
+        attention = torch.matmul(query, key.transpose(-2, -1)) / self.head_dim ** 0.5
+        attention = torch.nn.functional.softmax(attention, dim=-1)
+        out = torch.matmul(attention, value).transpose(1, 2).contiguous().view(batch_size, -1, self.query_dim)
+        return self.out(out)
\ No newline at end of file