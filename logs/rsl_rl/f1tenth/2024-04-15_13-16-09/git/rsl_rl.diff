--- git status ---
On branch attention
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   rsl_rl/modules/actor_critic_recurrent.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/rsl_rl/modules/actor_critic_recurrent.py b/rsl_rl/modules/actor_critic_recurrent.py
index 6321ec5..e9477a5 100644
--- a/rsl_rl/modules/actor_critic_recurrent.py
+++ b/rsl_rl/modules/actor_critic_recurrent.py
@@ -25,6 +25,10 @@ class ActorCriticRecurrent(ActorCritic):
         rnn_hidden_size=256,
         rnn_num_layers=1,
         init_noise_std=1.0,
+        attention=False,
+        attention_type=None,
+        attention_dims=None,
+        attention_heads=None,
         **kwargs,
     ):
         if kwargs:
@@ -42,29 +46,69 @@ class ActorCriticRecurrent(ActorCritic):
             init_noise_std=init_noise_std,
         )
 
+        print("**************************************************************************************")
+        print("* ATTENTION TIME | ATTENTION TIME | ATTENTION TIME | ATTENTION TIME | ATTENTION TIME *")
+        print("**************************************************************************************")
+
+        self.use_attention = attention
         activation = get_activation(activation)
 
+        if self.use_attention:
+            assert attention_dims is not None, "Attention dimensions must be provided if attention is enabled"
+            self.cross_attention = CrossAttention(
+                query_dim=attention_dims[0],
+                key_dim=attention_dims[1],
+                value_dim=attention_dims[2],
+                num_heads=attention_heads,
+            )
+
         self.memory_a = Memory(num_actor_obs, type=rnn_type, num_layers=rnn_num_layers, hidden_size=rnn_hidden_size)
         self.memory_c = Memory(num_critic_obs, type=rnn_type, num_layers=rnn_num_layers, hidden_size=rnn_hidden_size)
 
         print(f"Actor RNN: {self.memory_a}")
         print(f"Critic RNN: {self.memory_c}")
+        if self.use_attention:
+            print(f"Attention: {self.cross_attention}")
 
     def reset(self, dones=None):
         self.memory_a.reset(dones)
         self.memory_c.reset(dones)
 
     def act(self, observations, masks=None, hidden_states=None):
-        input_a = self.memory_a(observations, masks, hidden_states)
-        return super().act(input_a.squeeze(0))
-
+        # input_a = self.memory_a(observations, masks, hidden_states)
+        # return super().act(input_a.squeeze(0))
+        actor_hidden = self.memory_a(observations, masks, hidden_states)
+        if self.use_attention:
+            # Assume critic_hidden is available or needs to be generated in a similar fashion to actor_hidden
+            critic_hidden = self.memory_c(observations, masks, hidden_states)  # This might need context-specific adjustment
+            actor_hidden = self.cross_attention(actor_hidden, critic_hidden, critic_hidden)
+
+        action_output = super().act(actor_hidden.squeeze(0))
+        return action_output
+
+   
     def act_inference(self, observations):
-        input_a = self.memory_a(observations)
-        return super().act_inference(input_a.squeeze(0))
+        # input_a = self.memory_a(observations)
+        # return super().act_inference(input_a.squeeze(0))
+        actor_hidden = self.memory_a(observations)
+        if self.use_attention:
+            critic_hidden = self.memory_c(observations)
+            actor_hidden = self.cross_attention(actor_hidden, critic_hidden, critic_hidden)
+
+        action_output = super().act_inference(actor_hidden.squeeze(0))
+        return action_output
 
     def evaluate(self, critic_observations, masks=None, hidden_states=None):
-        input_c = self.memory_c(critic_observations, masks, hidden_states)
-        return super().evaluate(input_c.squeeze(0))
+        # input_c = self.memory_c(critic_observations, masks, hidden_states)
+        # return super().evaluate(input_c.squeeze(0))
+        critic_hidden = self.memory_c(critic_observations, masks, hidden_states)
+        if self.use_attention:
+            actor_hidden = self.memory_a(critic_observations, masks, hidden_states)
+            critic_hidden = self.cross_attention(critic_hidden, actor_hidden, actor_hidden)
+        
+        value_output = super().evaluate(critic_hidden.squeeze(0))
+        return value_output
+
 
     def get_hidden_states(self):
         return self.memory_a.hidden_states, self.memory_c.hidden_states
@@ -95,3 +139,29 @@ class Memory(torch.nn.Module):
         # When the RNN is an LSTM, self.hidden_states_a is a list with hidden_state and cell_state
         for hidden_state in self.hidden_states:
             hidden_state[..., dones, :] = 0.0
+
+
+class CrossAttention(nn.Module):
+    def __init__(self, query_dim, key_dim, value_dim, num_heads=1):
+        super().__init__()
+        self.query_dim = query_dim
+        self.key_dim = key_dim
+        self.value_dim = value_dim
+        self.num_heads = num_heads
+        self.head_dim = query_dim // num_heads
+
+        self.query = nn.Linear(query_dim, query_dim)
+        self.key = nn.Linear(key_dim, key_dim)
+        self.value = nn.Linear(value_dim, value_dim)
+        self.out = nn.Linear(value_dim, query_dim)
+
+    def forward(self, query, key, value):
+        batch_size = query.size(0)
+        query = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
+        key = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
+        value = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
+
+        _attention = torch.matmul(query, key.transpose(-2, -1)) / self.head_dim ** 0.5
+        _attention = torch.nn.functional.softmax(_attention, dim=-1)
+        out = torch.matmul(_attention, value).transpose(1, 2).contiguous().view(batch_size, -1, self.query_dim)
+        return self.out(out)
\ No newline at end of file