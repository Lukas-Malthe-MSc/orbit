--- git status ---
On branch transformer
Your branch is up to date with 'origin/transformer'.

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   rsl_rl/algorithms/ppo.py
	modified:   rsl_rl/modules/__init__.py
	modified:   rsl_rl/modules/actor_critic_recurrent.py
	deleted:    rsl_rl/modules/temp.py
	modified:   rsl_rl/runners/on_policy_runner.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	rsl_rl/modules/actor_critic_lidar_cnn.py
	rsl_rl/modules/actor_critic_self_attention.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/rsl_rl/algorithms/ppo.py b/rsl_rl/algorithms/ppo.py
index 233739a..e5d0818 100644
--- a/rsl_rl/algorithms/ppo.py
+++ b/rsl_rl/algorithms/ppo.py
@@ -103,8 +103,8 @@ class PPO:
         mean_surrogate_loss = 0
         if self.actor_critic.is_recurrent:
             generator = self.storage.reccurent_mini_batch_generator(self.num_mini_batches, self.num_learning_epochs)
-        elif self.actor_critic.is_transformer:
-            generator = self.storage.transformer_mini_batch_generator(self.num_mini_batches, self.num_learning_epochs)
+        # elif self.actor_critic.is_transformer:
+        #     generator = self.storage.transformer_mini_batch_generator(self.num_mini_batches, self.num_learning_epochs)
         else:
             generator = self.storage.mini_batch_generator(self.num_mini_batches, self.num_learning_epochs)
         for (
diff --git a/rsl_rl/modules/__init__.py b/rsl_rl/modules/__init__.py
index 44555b0..15c714b 100644
--- a/rsl_rl/modules/__init__.py
+++ b/rsl_rl/modules/__init__.py
@@ -7,4 +7,7 @@ from .actor_critic import ActorCritic
 from .actor_critic_recurrent import ActorCriticRecurrent
 from .normalizer import EmpiricalNormalization
 from .actor_critic_transformer import ActorCriticTransformer
-__all__ = ["ActorCritic", "ActorCriticRecurrent", "EmpiricalNormalization", "ActorCriticTransformer"]
+from .actor_critic_self_attention import ActorCriticSelfAttention
+from .actor_critic_lidar_cnn import ActorCriticLidarCnn
+
+__all__ = ["ActorCritic", "ActorCriticRecurrent", "EmpiricalNormalization", "ActorCriticTransformer", "ActorCriticSelfAttention", "ActorCriticLidarCnn"]
diff --git a/rsl_rl/modules/actor_critic_recurrent.py b/rsl_rl/modules/actor_critic_recurrent.py
index b9999d6..b74f8c3 100644
--- a/rsl_rl/modules/actor_critic_recurrent.py
+++ b/rsl_rl/modules/actor_critic_recurrent.py
@@ -1,6 +1,3 @@
-#  Copyright 2021 ETH Zurich, NVIDIA CORPORATION
-#  SPDX-License-Identifier: BSD-3-Clause
-
 from __future__ import annotations
 
 import torch
@@ -79,27 +76,29 @@ class Memory(torch.nn.Module):
         rnn_cls = nn.GRU if type.lower() == "gru" else nn.LSTM
         self.rnn = rnn_cls(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)
         self.hidden_states = None
-        self.attention = MultiHeadSelfAttention(input_size=hidden_size, num_heads=4, attention_size=hidden_size)
-        self.layer_norm = nn.LayerNorm(hidden_size)  # Adding LayerNorm layer
 
     def forward(self, input, masks=None, hidden_states=None):
         batch_mode = masks is not None
+
+        """
+        # Adjust input shape for Conv1D [batch, features, seq_len]
+        input = input.permute(1, 2, 0)
+        # Conv1D processing
+        conv_output = self.conv1d(input)
+        # Adjust output shape for RNN [seq_len, batch, features]
+        rnn_input = conv_output.permute(2, 0, 1)
+        """
+        
         if batch_mode:
             # batch mode (policy update): need saved hidden states
             if hidden_states is None:
                 raise ValueError("Hidden states not passed to memory module during policy update")
             out, _ = self.rnn(input, hidden_states)
-            attn_out, _ = self.attention(out, out, out)
-            out = out + attn_out
             out = unpad_trajectories(out, masks)
 
         else:
             # inference mode (collection): use hidden states of last step
             out, self.hidden_states = self.rnn(input.unsqueeze(0), self.hidden_states)
-            attn_out, _ = self.attention(out, out, out)
-            out = out + attn_out
-
-        out = self.layer_norm(out)
 
         return out
 
@@ -107,41 +106,4 @@ class Memory(torch.nn.Module):
         # When the RNN is an LSTM, self.hidden_states_a is a list with hidden_state and cell_state
         for hidden_state in self.hidden_states:
             hidden_state[..., dones, :] = 0.0
-            
-class MultiHeadSelfAttention(nn.Module):
-    def __init__(self, input_size, num_heads, attention_size):
-        super().__init__()
-        assert attention_size % num_heads == 0, "Attention size must be divisible by the number of heads."
-        self.num_heads = num_heads
-        self.head_size = attention_size // num_heads
-        
-        # Linear transformations for query, key, and value for each head
-        self.query_linear = nn.Linear(input_size, attention_size, bias=False)
-        self.key_linear = nn.Linear(input_size, attention_size, bias=False)
-        self.value_linear = nn.Linear(input_size, attention_size, bias=False)
-        
-        # Final linear transformation after concatenating heads
-        self.output_linear = nn.Linear(attention_size, input_size)
-        
-        self.scale = 1.0 / (self.head_size ** 0.5)
-
-    def forward(self, query, key, value, mask=None):
-        batch_size = query.size(0)
-        
-        # Linear transformations for query, key, and value for each head
-        query = self.query_linear(query).view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)
-        key = self.key_linear(key).view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)
-        value = self.value_linear(value).view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)
-        
-        # Compute scaled dot-product attention for each head
-        scores = torch.matmul(query, key.transpose(-2, -1)) * self.scale
-        if mask is not None:
-            scores = scores.masked_fill(mask == 0, float("-inf"))
-        attn_weights = torch.softmax(scores, dim=-1)
-        attn_output = torch.matmul(attn_weights, value)
-        
-        # Concatenate the outputs of all heads and apply final linear transformation
-        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_size)
-        output = self.output_linear(attn_output)
-        
-        return output, attn_weights
\ No newline at end of file
+            
\ No newline at end of file
diff --git a/rsl_rl/modules/temp.py b/rsl_rl/modules/temp.py
deleted file mode 100644
index b68c53b..0000000
--- a/rsl_rl/modules/temp.py
+++ /dev/null
@@ -1,88 +0,0 @@
-import torch
-import torch.nn as nn
-
-from rsl_rl.modules.actor_critic import ActorCritic, get_activation
-
-class ActorCriticTransformer(ActorCritic):
-    def __init__(
-        self,
-        num_actor_obs,  # input size of actor transformer
-        num_critic_obs,  # input size of critic transformer
-        num_actions,
-        actor_hidden_dims=[256, 256, 256],
-        critic_hidden_dims=[256, 256, 256],
-        activation="elu",
-        init_noise_std=1.0,
-        num_heads=4,
-        transformer_layers=3,
-        transformer_dim=512,
-        **kwargs,
-    ):
-        if kwargs:
-            print("ActorCriticTransformer.__init__ got unexpected arguments, which will be ignored: " + str(kwargs.keys()))
-
-        # Initialize base ActorCritic with adapted input dimensions (from transformers)
-        super(ActorCriticTransformer, self).__init__(
-            num_actor_obs=transformer_dim,  # output dim of transformer
-            num_critic_obs=transformer_dim,
-            num_actions=num_actions,
-            actor_hidden_dims=actor_hidden_dims,
-            critic_hidden_dims=critic_hidden_dims,
-            activation=activation,
-            init_noise_std=init_noise_std,
-        )
-        
-        activation = get_activation(activation)
-        self.is_transformer = True
-        
-        # Initialize transformers
-        self.transformer_a = Transformer(num_actor_obs, transformer_dim, num_heads, transformer_layers, transformer_dim)
-        self.transformer_c = Transformer(num_critic_obs, transformer_dim, num_heads, transformer_layers, transformer_dim)
-        
-        print(f"Actor Transformer: {self.transformer_a}")
-        print(f"Critic Transformer: {self.transformer_c}")
-
-
-    def act(self, observations, **kwargs):
-        input_a = self.transformer_a(observations)
-        return super().act(input_a)
-
-    def act_inference(self, observations):
-        input_a = self.transformer_a(observations)
-        return super().act_inference(input_a)
-
-    def evaluate(self, critic_observations, **kwargs):
-        input_c = self.transformer_c(critic_observations)
-        return super().evaluate(input_c)
-    
-    def reset(self, dones=None):
-        # Reset transformers if needed (likely not necessary for standard transformers)
-        self.transformer_a.reset(dones)
-        self.transformer_c.reset(dones)
-
-
-class Transformer(nn.Module):
-    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim):
-        super(Transformer, self).__init__()
-        self.input_projection = nn.Linear(input_dim, model_dim)
-        self.transformer_encoder = nn.TransformerEncoder(
-            nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads),
-            num_layers=num_layers
-        )
-        self.output_projection = nn.Linear(model_dim, output_dim)
-        
-        self.x_buffer = None
-
-    def forward(self, x):
-        # print(f"Input shape: {x.shape}")
-        x = self.input_projection(x)
-        x = x.unsqueeze(0)  # Add batch dimension
-        x = self.transformer_encoder(x)
-        x = x.squeeze(0)
-        x = self.output_projection(x)
-        print(f"Output shape: {x.shape}")
-        return x
-
-    def reset(self, dones=None):
-        # No state to reset in standard transformer, but method exists for interface compatibility
-        pass
diff --git a/rsl_rl/runners/on_policy_runner.py b/rsl_rl/runners/on_policy_runner.py
index 3188f4e..9086c3a 100644
--- a/rsl_rl/runners/on_policy_runner.py
+++ b/rsl_rl/runners/on_policy_runner.py
@@ -13,7 +13,7 @@ from torch.utils.tensorboard import SummaryWriter as TensorboardSummaryWriter
 import rsl_rl
 from rsl_rl.algorithms import PPO
 from rsl_rl.env import VecEnv
-from rsl_rl.modules import ActorCritic, ActorCriticRecurrent, EmpiricalNormalization, ActorCriticTransformer
+from rsl_rl.modules import ActorCritic, ActorCriticRecurrent, EmpiricalNormalization, ActorCriticSelfAttention, ActorCriticLidarCnn
 from rsl_rl.utils import store_code_state
 
 
@@ -34,7 +34,7 @@ class OnPolicyRunner:
         else:
             num_critic_obs = num_obs
         actor_critic_class = eval(self.policy_cfg.pop("class_name"))  # ActorCritic
-        actor_critic: ActorCritic | ActorCriticRecurrent | ActorCriticTransformer= actor_critic_class(
+        actor_critic: ActorCritic | ActorCriticRecurrent | ActorCriticSelfAttention | ActorCriticLidarCnn = actor_critic_class(
             num_obs, num_critic_obs, self.env.num_actions, **self.policy_cfg
         ).to(self.device)
         alg_class = eval(self.alg_cfg.pop("class_name"))  # PPO